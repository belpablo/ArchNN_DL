{"cells":[{"cell_type":"markdown","metadata":{"id":"9O4Vm-vIiLwk"},"source":["# Walker2D\n","\n","You aim in this task is to train the agent to win in Walker2D game with Actor-Critic, Advantage Actor Critic (A2C), Trust-region Policy Optimization (TRPO) or Proximal Policy Optimization (PPO). \n","To solve the task feel free to transform the state and reward from the environment.\n","\n","**Scoring**: Calculating the average reward for 50 episodes. You goal is to gain more than 1000 points.\n","\n","**Submission format**: send you notebook and trained model in **zipped** folder.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NwtoZiBNZ7kG","executionInfo":{"status":"ok","timestamp":1679840434380,"user_tz":-180,"elapsed":30724,"user":{"displayName":"Pavel Belenko","userId":"01154748966313521845"}},"outputId":"b534e5bd-86d8-4be3-d89d-91d8cd1d891c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive/')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rXDVWmoEgt55"},"outputs":[],"source":["! pip install PyBullet >> None"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q0mS0Xd9hRIk","executionInfo":{"status":"ok","timestamp":1679840446781,"user_tz":-180,"elapsed":3574,"user":{"displayName":"Pavel Belenko","userId":"01154748966313521845"}},"outputId":"c8ea6192-e9ac-41de-dbdf-ccd60ad7d5ac"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/gym/envs/registration.py:440: UserWarning: \u001b[33mWARN: The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead.\u001b[0m\n","  logger.warn(\n"]}],"source":["import pybullet_envs\n","from gym import make\n","import numpy as np\n","import torch\n","from torch import nn\n","from torch.distributions import Normal\n","from torch.nn import functional as F\n","from torch.optim import Adam\n","import random\n","import os\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3CZ9jz9qJB3f","executionInfo":{"status":"ok","timestamp":1679840446781,"user_tz":-180,"elapsed":8,"user":{"displayName":"Pavel Belenko","userId":"01154748966313521845"}},"outputId":"1e5ed5f9-7a31-4a87-9597-2b54fc567643"},"outputs":[{"output_type":"stream","name":"stdout","text":["Device set to : Tesla T4\n"]}],"source":["device = torch.device('cpu')\n","\n","if(torch.cuda.is_available()):\n","    device = torch.device('cuda:0')\n","    torch.cuda.empty_cache()\n","    print(\"Device set to : \" + str(torch.cuda.get_device_name(device)))\n","else:\n","    print(\"Device set to : cpu\")"]},{"cell_type":"markdown","metadata":{"id":"8aUR0Fy4g5Bq"},"source":["# Train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9wHwdfYyg7JE"},"outputs":[],"source":["ENV_NAME = \"Walker2DBulletEnv-v0\"\n","\n","LAMBDA = 0.95\n","GAMMA = 0.99\n","\n","ACTOR_LR = 2e-5\n","CRITIC_LR = 1e-5\n","\n","CLIP = 0.2\n","ENTROPY_COEF = 1e-2\n","BATCHES_PER_UPDATE = 2048\n","BATCH_SIZE = 64\n","\n","MIN_TRANSITIONS_PER_UPDATE = 2048\n","MIN_EPISODES_PER_UPDATE = 4\n","\n","ITERATIONS = 2000\n","\n","def compute_lambda_returns_and_gae(trajectory):\n","    lambda_returns = []\n","    gae = []\n","    last_lr = 0.\n","    last_v = 0.\n","    for _, _, r, _, v in reversed(trajectory):\n","        ret = r + GAMMA * (last_v * (1 - LAMBDA) + last_lr * LAMBDA)\n","        last_lr = ret\n","        last_v = v\n","        lambda_returns.append(last_lr)\n","        gae.append(last_lr - v)\n","\n","    # Each transition contains state, action, old action probability, value estimation and advantage estimation\n","    return [(s, a, p, v, adv) for (s, a, _, p, _), v, adv in zip(trajectory, reversed(lambda_returns), reversed(gae))]\n","\n","\n","\n","class Actor(nn.Module):\n","    def __init__(self, state_dim, action_dim):\n","        super().__init__()\n","        # Advice: use same log_sigma for all states to improve stability\n","        # You can do this by defining log_sigma as nn.Parameter(torch.zeros(...))\n","        ''' YOUR CODE HERE '''\n","        self.model =  nn.Sequential(\n","              nn.Linear(state_dim, 256),\n","              nn.ELU(),\n","              nn.Linear(256, 256),\n","              nn.ELU(),\n","              nn.Linear(256, action_dim))\n","        self.sigma = nn.Parameter(torch.ones(action_dim))\n","\n","    def compute_proba(self, state, action):\n","        # Returns probability of action according to current policy and distribution of actions\n","        ''' YOUR CODE HERE '''\n","        mu = self.model(state)\n","        dist = Normal(mu, torch.exp(self.sigma))\n","        prob = torch.exp(dist.log_prob(action).sum(-1))\n","        return prob, dist\n","\n","    def act(self, state):\n","        # Returns an action (with tanh), not-transformed action (without tanh) and distribution of non-transformed actions\n","        # Remember: agent is not deterministic, sample actions from distribution (e.g. Gaussian)\n","        ''' YOUR CODE HERE '''\n","        mu = self.model(state)\n","        dist = Normal(mu, torch.exp(self.sigma))\n","        action = dist.sample()\n","\n","        return torch.tanh(action), action, dist\n","\n","\n","class Critic(nn.Module):\n","    def __init__(self, state_dim):\n","        super().__init__()\n","        self.model = nn.Sequential(\n","            nn.Linear(state_dim, 256),\n","            nn.ELU(),\n","            nn.Linear(256, 256),\n","            nn.ELU(),\n","            nn.Linear(256, 1)\n","        )\n","\n","    def get_value(self, state):\n","        return self.model(state)\n","\n","\n","class PPO:\n","    def __init__(self, state_dim, action_dim):\n","        self.actor = Actor(state_dim, action_dim)\n","        self.critic = Critic(state_dim)\n","        self.actor_optim = Adam(self.actor.parameters(), ACTOR_LR)\n","        self.critic_optim = Adam(self.critic.parameters(), CRITIC_LR)\n","\n","    def update(self, trajectories):\n","        transitions = [t for traj in trajectories for t in traj] # Turn a list of trajectories into list of transitions\n","        state, action, old_prob, target_value, advantage = zip(*transitions)\n","        state = np.array(state)\n","        action = np.array(action)\n","        old_prob = np.array(old_prob)\n","        target_value = np.array(target_value)\n","        advantage = np.array(advantage)\n","        advnatage = (advantage - advantage.mean()) / (advantage.std() + 1e-8)\n","\n","\n","        for _ in range(BATCHES_PER_UPDATE):\n","            idx = np.random.randint(0, len(transitions), BATCH_SIZE) # Choose random batch\n","\n","            s = torch.tensor(state[idx]).float()\n","            a = torch.tensor(action[idx]).float()\n","            op = torch.tensor(old_prob[idx]).float() # Probability of the action in state s.t. old policy\n","            v = torch.tensor(target_value[idx]).float() # Estimated by lambda-returns\n","            adv = torch.tensor(advantage[idx]).float() # Estimated by generalized advantage estimation\n","\n","            ''' YOUR CODE HERE '''\n","            # TODO: Update actor here\n","            # calculate ratios\n","            new_prob, dist = self.actor.compute_proba(s, a)\n","            ratio = new_prob / op\n","\n","            # actor_loss\n","            surr_loss = ratio * adv\n","            clipped_surr_loss = (\n","                torch.clamp(ratio, 1.0 - CLIP, 1.0 + CLIP) * adv\n","            )\n","\n","            # entropy\n","            entropy = dist.entropy().mean()\n","\n","            actor_loss = (\n","                - torch.min(surr_loss, clipped_surr_loss).mean()\n","                - entropy * ENTROPY_COEF\n","            )\n","\n","            self.actor_optim.zero_grad()\n","            actor_loss.backward(retain_graph=True)\n","            self.actor_optim.step()\n","\n","\n","            # TODO: Update critic here\n","            # critic_loss\n","            value = self.critic.get_value(s).flatten()\n","            # critic_loss = (v - value).pow(2).mean()\n","            critic_loss = F.smooth_l1_loss(value, v)\n","\n","            # train critic\n","            self.critic_optim.zero_grad()\n","            critic_loss.backward(retain_graph=True)\n","            self.critic_optim.step()\n","\n","\n","\n","    def get_value(self, state):\n","        with torch.no_grad():\n","            state = torch.tensor(np.array([state])).float()\n","            value = self.critic.get_value(state)\n","        return value.cpu().item()\n","\n","    def act(self, state):\n","        with torch.no_grad():\n","            state = torch.tensor(np.array([state])).float()\n","            action, pure_action, distr = self.actor.act(state)\n","            prob = torch.exp(distr.log_prob(pure_action).sum(-1))\n","        return action.cpu().numpy()[0], pure_action.cpu().numpy()[0], prob.cpu().item()\n","\n","    def save(self):\n","        torch.save(self.actor, \"./drive/MyDrive/Colab Notebooks/agent.pkl\")\n","\n","\n","\n","def evaluate_policy(env, agent, episodes=5):\n","    returns = []\n","    for _ in range(episodes):\n","        done = False\n","        state = env.reset()\n","        total_reward = 0.\n","\n","        while not done:\n","            state, reward, done, _ = env.step(agent.act(state)[0])\n","            total_reward += reward\n","        returns.append(total_reward)\n","    return returns\n","\n","\n","def sample_episode(env, agent):\n","    s = env.reset()\n","    d = False\n","    trajectory = []\n","    while not d:\n","        a, pa, p = agent.act(s)\n","        v = agent.get_value(s)\n","        ns, r, d, _ = env.step(a)\n","        trajectory.append((s, pa, r, p, v))\n","        s = ns\n","    return compute_lambda_returns_and_gae(trajectory)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ELe0GF4uhKYD","outputId":"ef10dde3-7554-41b0-99b0-39d5ce429271"},"outputs":[{"output_type":"stream","name":"stdout","text":["Step: 20, Reward mean: 45.912436342317655, Reward std: 17.92524886797264, Episodes: 1476, Steps: 41705\n","Step: 40, Reward mean: 32.94913055201498, Reward std: 23.631972273781972, Episodes: 2010, Steps: 83392\n","Step: 60, Reward mean: 308.3730948049393, Reward std: 185.00918741017782, Episodes: 2445, Steps: 125868\n","Step: 80, Reward mean: 269.042886738331, Reward std: 174.13413386933723, Episodes: 2612, Steps: 174260\n","Step: 100, Reward mean: 282.50284344558304, Reward std: 248.40337332422166, Episodes: 2718, Steps: 223640\n","Step: 120, Reward mean: 238.351177589216, Reward std: 190.13083762612902, Episodes: 2813, Steps: 275945\n","Step: 140, Reward mean: 423.32546900915656, Reward std: 248.11145783740733, Episodes: 2905, Steps: 330158\n","Step: 160, Reward mean: 619.8637548654264, Reward std: 12.238802480595247, Episodes: 2999, Steps: 386723\n","Step: 180, Reward mean: 450.21585017899395, Reward std: 241.09895705086342, Episodes: 3084, Steps: 448670\n","Step: 200, Reward mean: 479.3695944158749, Reward std: 213.19809916690727, Episodes: 3166, Steps: 508383\n","Step: 220, Reward mean: 333.1610527024473, Reward std: 276.27995615349334, Episodes: 3249, Steps: 573353\n","Step: 240, Reward mean: 659.9592217766701, Reward std: 12.600364651801835, Episodes: 3340, Steps: 630956\n","Step: 260, Reward mean: 204.67206441694196, Reward std: 84.59377623293621, Episodes: 3426, Steps: 692211\n","Step: 280, Reward mean: 512.189481359561, Reward std: 238.50985934931407, Episodes: 3520, Steps: 752266\n","Step: 300, Reward mean: 280.042388529371, Reward std: 227.6410619663804, Episodes: 3603, Steps: 812203\n","Step: 320, Reward mean: 412.02510200509204, Reward std: 254.53527331219215, Episodes: 3687, Steps: 867616\n","Step: 340, Reward mean: 370.6627584358038, Reward std: 154.0877513803149, Episodes: 3798, Steps: 915364\n","Step: 360, Reward mean: 273.0189457023222, Reward std: 197.51273724188286, Episodes: 3903, Steps: 964153\n","Step: 380, Reward mean: 342.41270817107966, Reward std: 176.3237854708179, Episodes: 4014, Steps: 1012376\n","Step: 400, Reward mean: 242.22026674686072, Reward std: 194.62727043905886, Episodes: 4136, Steps: 1061784\n","Step: 420, Reward mean: 493.05536911187966, Reward std: 399.825832413858, Episodes: 4223, Steps: 1112541\n","Step: 440, Reward mean: 372.2332770630708, Reward std: 333.6690842700893, Episodes: 4312, Steps: 1166035\n","Step: 460, Reward mean: 409.2156210590752, Reward std: 288.51686002689434, Episodes: 4412, Steps: 1216300\n","Step: 480, Reward mean: 405.27728818926334, Reward std: 310.3730269233773, Episodes: 4517, Steps: 1267341\n","Step: 500, Reward mean: 705.1008829455105, Reward std: 200.5416628109973, Episodes: 4604, Steps: 1318842\n","Step: 520, Reward mean: 591.4301629241867, Reward std: 305.1976229504508, Episodes: 4706, Steps: 1370206\n","Step: 540, Reward mean: 602.8017363981562, Reward std: 361.3632075591827, Episodes: 4802, Steps: 1421208\n","Step: 560, Reward mean: 556.6547351634042, Reward std: 342.66729742638125, Episodes: 4906, Steps: 1470467\n","Step: 580, Reward mean: 806.8893846725864, Reward std: 483.00189447989766, Episodes: 4995, Steps: 1522232\n","Step: 600, Reward mean: 655.4044558732987, Reward std: 417.22484797905037, Episodes: 5090, Steps: 1571937\n","Step: 620, Reward mean: 967.2427990037877, Reward std: 316.0643269502952, Episodes: 5171, Steps: 1628784\n","Step: 640, Reward mean: 762.4725881563352, Reward std: 531.0435546094062, Episodes: 5265, Steps: 1686258\n","Step: 660, Reward mean: 633.3652173770834, Reward std: 452.2294916812948, Episodes: 5377, Steps: 1736673\n","Step: 680, Reward mean: 661.9346659253727, Reward std: 464.86056817416994, Episodes: 5479, Steps: 1784489\n","Step: 700, Reward mean: 620.540466130335, Reward std: 353.1839313547845, Episodes: 5573, Steps: 1837106\n","Step: 720, Reward mean: 407.5446295540676, Reward std: 451.2833517794104, Episodes: 5667, Steps: 1895011\n","Step: 740, Reward mean: 1012.217223455066, Reward std: 394.7598519928189, Episodes: 5759, Steps: 1949318\n","Step: 760, Reward mean: 918.4145924130795, Reward std: 400.1640328729138, Episodes: 5851, Steps: 2001517\n","Step: 780, Reward mean: 844.8858661948132, Reward std: 406.6437607208252, Episodes: 5935, Steps: 2056744\n","Step: 800, Reward mean: 652.1513330398345, Reward std: 469.6878447567484, Episodes: 6030, Steps: 2107874\n","Step: 820, Reward mean: 1018.1077499738446, Reward std: 321.797594616296, Episodes: 6123, Steps: 2160959\n","Step: 840, Reward mean: 761.3514611287794, Reward std: 450.5772269468617, Episodes: 6218, Steps: 2211835\n","Step: 860, Reward mean: 466.4447506652209, Reward std: 370.65786267552124, Episodes: 6315, Steps: 2262616\n","Step: 880, Reward mean: 829.5278165840843, Reward std: 253.9975353989644, Episodes: 6407, Steps: 2315576\n","Step: 900, Reward mean: 1062.312737798533, Reward std: 383.4030133116667, Episodes: 6492, Steps: 2373025\n","Step: 920, Reward mean: 612.3485358974065, Reward std: 506.74910594306544, Episodes: 6588, Steps: 2423731\n","Step: 940, Reward mean: 1035.7010906012285, Reward std: 399.82740444241927, Episodes: 6683, Steps: 2473360\n","Step: 960, Reward mean: 951.8643208728878, Reward std: 412.33143595176375, Episodes: 6773, Steps: 2527219\n","Step: 980, Reward mean: 699.902539684432, Reward std: 381.4997947695155, Episodes: 6866, Steps: 2581203\n","Step: 1000, Reward mean: 654.6660855711955, Reward std: 421.5467316428021, Episodes: 6959, Steps: 2638597\n","Step: 1020, Reward mean: 778.8390494482469, Reward std: 440.7185008896972, Episodes: 7062, Steps: 2689418\n","Step: 1040, Reward mean: 974.9811005940595, Reward std: 255.3253813333019, Episodes: 7159, Steps: 2742098\n","Step: 1060, Reward mean: 920.8327563426379, Reward std: 456.3237338587762, Episodes: 7259, Steps: 2793561\n"]}],"source":["\n","env = make(ENV_NAME)\n","ppo = PPO(state_dim=env.observation_space.shape[0], action_dim=env.action_space.shape[0])\n","state = env.reset()\n","episodes_sampled = 0\n","steps_sampled = 0\n","\n","for i in range(ITERATIONS):\n","    trajectories = []\n","    steps_ctn = 0\n","\n","    while len(trajectories) < MIN_EPISODES_PER_UPDATE or steps_ctn < MIN_TRANSITIONS_PER_UPDATE:\n","        traj = sample_episode(env, ppo)\n","        steps_ctn += len(traj)\n","        trajectories.append(traj)\n","    episodes_sampled += len(trajectories)\n","    steps_sampled += steps_ctn\n","\n","    ppo.update(trajectories)\n","\n","    if (i + 1) % (ITERATIONS//100) == 0:\n","        rewards = evaluate_policy(env, ppo, 5)\n","        print(f\"Step: {i+1}, Reward mean: {np.mean(rewards)}, Reward std: {np.std(rewards)}, Episodes: {episodes_sampled}, Steps: {steps_sampled}\")\n","        ppo.save()\n","        "]},{"cell_type":"markdown","metadata":{"id":"hne6AOIohb1W"},"source":["# Agent"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5pdqXy1lheAP"},"outputs":[],"source":["\n","class Agent:\n","    def __init__(self):\n","        self.model = torch.load(__file__[:-8] + \"/agent.pkl\")\n","\n","    def act(self, state):\n","        with torch.no_grad():\n","            state = torch.tensor(np.array(state)).float()\n","            ''' YOUR CODE HERE '''\n","            actions = self.model(state)\n","            return np.argmax(actions.cpu().numpy())\n","\n","    def reset(self):\n","        pass"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lTw9KJXK6I3B"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1DTgbjoQ7uSaZ6OLy0pG7g9A8eo5QNW9Z","timestamp":1679307285212},{"file_id":"11RTiEG_LSemRbenGeaUmFcfZQmFMFvLC","timestamp":1678812151703},{"file_id":"1fdAho-3oO-l7RH2op5c8-NHb3AnRnwUR","timestamp":1669378791619}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}